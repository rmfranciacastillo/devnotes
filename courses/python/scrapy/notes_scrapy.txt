Notes: Scrapy
==============

Most of the notes are in the code but here are some pointers.

** It is recommended to install scrapy in virtual environment.

python3 -m venv env
source env/bin/activate
pip install scrapy

Entering the shell for scrapy.

Commands:
----------
scrapy

  ** Global **
  startproject <projectName>    # Starts the project scaffold
  genspider <spider> <url>      # generates a new spider


  ** project only **
  crawl <spiderName>            # Crawls the website with the spider name
  list                          # Shows a list of available spiders
  check                         # Run contract checks
  bench                         # Runs a benchmarking test


Example:

	# SHELL
  # Get the title from quotes
  scrapy shell 'http://quotes.toscrape.com/page/1/'

  >> fetch('www.example.com') # Fetch the urls

  # Add the '::text' to get the text inside the title
  response.css('title::text').getall()
  response.css('title::text').get()

  # You can also use re() to use regular expressions
  response.css('title::text').re(r'Quotes.*')

  # Scrapy also use xPath (CSS seems easier but ok....)
  response.xpath('//title').get()

  # Creating a Dictionary from scraper
  post = response.css('div.post-item')[0]
  title = post.css('.post-header h2 a::text')[0].get()
  date = post.css('.post-header a::text')[1].get()
  author = post.css('.post-header a::text')[2].get()

  for post in response.css('div.post-item'):
    title = post.css('.post-header h2 a::text')[0].get()
    date = post.css('.post-header a::text')[1].get()
    author = post.css('.post-header a::text')[2].get()
    print(dict(title=title, date=date, author=author))


Command line Tool
------------------
Can have multiple settings inside the project.

commands:
**********

  [settings]
  default = myproject1.settings
  project1 = myproject1.settings
  project2 = myproject2.settings


  # Generating Spider from templates
  genspider [-t template] <spider> <url>      # generates a new spider
  scrapy genspider -l                         # Shows the available templates


XPath:
*******

Great resource for XPath querying
https://devhints.io/xpath


Spider:
--------

name                          # Holds the name of the spider

allowed_domains               # Setup the allowed domains to scrape
                              # OffsiteMiddleware has to be enabled

start_urls                    # list of URLs to crawl

custom_settings               # dictionary of settings. Ex: AWS_ACCESS_KEY_ID

from_crawler                  # Acts as a proxy to __init() and has arguments
                              # (crawler, *args, **kwargs)

start_requests                # When you want to start scraping a site that needs
                              # a POST request to begin. Ex: Login to a protected
                              # page

CrawlSpider
************

rules                         # List of 1 or more rules that determines a certain
                              # behaviour when crawling the site. Rules are Objects

  # ex: Class Rule
  class scrapy.spiders.Rule(
      link_extractor=None,
      callback=None,
      cb_kwargs=None,
      follow=None,
      process_links=None,
      process_request=None,
      errback=None
  )

  # example definition:
    rules = (
      # Extract links matching 'category.php' (but not matching 'subsection.php')
      # and follow links from them (since no callback means follow=True by default).
      Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

      # Extract links matching 'item.php' and parse them with the spider's method parse_item
      Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

# Note: Other Spiders may cover CSV or XML files. For example, XMLFeedSpider.


Selectors:
------------

# Returns Boolean value.
>>> response.xpath('//div[@id="not-exists"]/text()').get() is None
True

# Default .get() with defaults
>>> response.xpath('//div[@id="not-exists"]/text()').get(default='not-found')
'not-found'

# can save the response in a Selector Object
from scrapy import Selector

  # .. inside parse
  sel = Selector(response)
  sel.xpath('xpath').get()


Items:
------
A great way to organize the data you are scraping

ex:
  # Declaring the Item
  import scrapy

  class Product(scrapy.Item):
      name = scrapy.Field()
      price = scrapy.Field()
      stock = scrapy.Field()
      tags = scrapy.Field()
      last_updated = scrapy.Field(serializer=str)

  # Declaring values for Product
  product = Product(name='Desktop PC', price=1000)

  # Retrieving data
  >>> product['name']
  Desktop PC
  >>> product.get('name')
  Desktop PC

  # Accessing populated values
  >>> product.keys()
  ['price', 'name']
  >>> product.items()
  [('price', 1000), ('name', 'Desktop PC')]

  # Dictionary
  >>> dict(product) # create a dict from all populated values
  {'price': 1000, 'name': 'Desktop PC'}


Item Loaders:
***************

Item loaders are an easier way to populate the items

  ex:
  from scrapy.loader import ItemLoader
  from myproject.items import Product

  def parse(self, response):
      l = ItemLoader(item=Product(), response=response)
      l.add_xpath('name', '//div[@class="product_name"]')
      l.add_xpath('name', '//div[@class="product_title"]')
      l.add_xpath('price', '//p[@id="price"]')
      l.add_css('stock', 'p#stock]')
      l.add_value('last_updated', 'today') # you can also use literal values
      return l.load_item()


// TODO
I) Project to scrape all the images
II) Use Rules() and Link



